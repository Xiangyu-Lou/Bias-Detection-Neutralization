{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load the dataset from a tab-separated file, handling potential errors\n",
    "try:\n",
    "    data = pd.read_csv(\n",
    "        'data/biased.full',\n",
    "        sep='\\t',\n",
    "        quotechar='\"',\n",
    "        names=[\"id\", \"src_tok\", \"tgt_tok\", \"src_raw\", \"tgt_raw\", \"src_POS_tags\", \"tgt_parse_tags\"],\n",
    "        on_bad_lines='skip'\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Sample 2000 random records from the data\n",
    "sampled_data = data.sample(n=2000, random_state=42)\n",
    "\n",
    "# Split sampled data into two parts for two different labels\n",
    "data_with_label_1 = sampled_data.iloc[:1000]\n",
    "data_with_label_0 = sampled_data.iloc[1000:]\n",
    "\n",
    "# Create DataFrame for label 1\n",
    "df_label_1 = pd.DataFrame({\n",
    "    'label': 1,\n",
    "    'source_text': data_with_label_1['src_raw'],\n",
    "    'target_text': data_with_label_1['tgt_raw']\n",
    "})\n",
    "\n",
    "# Create DataFrame for label 0, with empty target_text\n",
    "df_label_0 = pd.DataFrame({\n",
    "    'label': 0,\n",
    "    'source_text': data_with_label_0['tgt_raw'],\n",
    "    'target_text': [''] * 1000\n",
    "})\n",
    "\n",
    "# Concatenate both DataFrames and shuffle them\n",
    "final_df = pd.concat([df_label_1, df_label_0])\n",
    "final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save the final sampled data to a CSV file\n",
    "final_df.to_csv('data/sampled_data.csv', index=False)\n",
    "\n",
    "# Split data into training and testing datasets\n",
    "train_data, test_data = train_test_split(final_df, test_size=0.4, random_state=42)\n",
    "\n",
    "# Reset the index of training and testing data and add a new column for indexing\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "train_data.insert(0, 'Sample Index', range(1, len(train_data) + 1))\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "test_data.insert(0, 'Sample Index', range(1, len(test_data) + 1))\n",
    "\n",
    "# Save the training and testing data to CSV files\n",
    "train_data.to_csv('data/train_data.csv', index=False)\n",
    "test_data.to_csv('data/test_data.csv', index=False)\n",
    "\n",
    "# Function to format data for chat-completion JSON lines\n",
    "def format_chat_completion(source_text, label, target_text=None):\n",
    "    '''\n",
    "    Formats the source text, label, and target text into a JSON line for chat-completion.\n",
    "    \n",
    "    Parameters:\n",
    "    source_text (str): The source text.\n",
    "    label (int): The label for the source text.\n",
    "    target_text (str): The target text for the source text.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list containing the formatted chat-completion JSON line.\n",
    "    '''\n",
    "    response = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant trained to identify and neutralize subjective bias. If you detect subjective bias, respond with 'This is subjective bias text.' and provide the neutralized text. If no subjective bias is detected, respond with 'This text does not contain detectable subjective bias.\"},\n",
    "        {\"role\": \"user\", \"content\": source_text},\n",
    "    ]\n",
    "    if label == 1:\n",
    "        assistant_response = \"This is subjective bias text.\"\n",
    "        if target_text:\n",
    "            assistant_response += f\" The neutralized text is: {target_text}\"\n",
    "    else:\n",
    "        assistant_response = \"This text does not contain detectable subjective bias.\"\n",
    "    response.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    return response\n",
    "\n",
    "# Function to write JSON lines to a file\n",
    "def write_to_jsonl(data, file_name):\n",
    "    with open(file_name, 'w') as jsonl_file:\n",
    "        for index, row in data.iterrows():\n",
    "            chat_completion = format_chat_completion(row['source_text'], row['label'], row.get('target_text', None))\n",
    "            entry = {\n",
    "                \"messages\": chat_completion\n",
    "            }\n",
    "            jsonl_file.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "# Create JSONL files for training and testing\n",
    "write_to_jsonl(train_data, 'data/fine_tuning_train_data.jsonl')\n",
    "write_to_jsonl(test_data, 'data/fine_tuning_test_data.jsonl')\n",
    "\n",
    "# Confirmation message after file creation\n",
    "print(\"JSONL files for training and testing have been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key from an environment variable for better security practices\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-#####\"\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Uncomment the following code block to upload your training data file.\n",
    "# It's recommended to add error handling and check the upload status.\n",
    "# try:\n",
    "#     file_response = client.files.create(\n",
    "#       file=open(\"data/fine_tuning_train_data.jsonl\", \"rb\"),\n",
    "#       purpose=\"fine-tune\"\n",
    "#     )\n",
    "#     print(\"File uploaded successfully. File ID:\", file_response.id)\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed to upload file: {e}\")\n",
    "\n",
    "# Create a fine-tuning job with the specified model and training file ID.\n",
    "# Include error handling to capture and react to API errors.\n",
    "try:\n",
    "    fine_tuning_job = client.fine_tuning.jobs.create(\n",
    "      training_file=\"file-#####\",  # Replace with actual file ID from the upload response\n",
    "      model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "    print(\"Fine-tuning job created successfully. Job ID:\", fine_tuning_job.id)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create fine-tuning job: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
